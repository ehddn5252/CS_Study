# Hadoop_core_components

학습 목표: 하둡에 어떠한 프로젝트가 있는 지 정리한다.

### 클러스터란?
- 클러스터는 저장이나 연산 등 하나 이상의 기능을 제공하기 위해 협력하는 서버의 조합을 말합니다.
- 클러스터의 사용자는 자신이 요청한 저장이나 연산 등의 작업이 클러스터 내에서 구체적으로 어느 장비나 장비의 조합에서 수행되는지 알 필요가 없습니다. 하지만 아키텍트나 관리자는 클러스터를 상세히 이해해야 합니다. 
![image](https://user-images.githubusercontent.com/51036842/188672785-7228d554-5343-4575-8765-025b6cfedc76.png)

- 위 그림에는 클러스터 레이아웃이 개괄적으로 그려져 있습니다.
- 클러스터에 사용되는 장비는 보통 마스터와 워커로 나눌 수 있습니다. 
- 워커 장비는 이름 그대로 실제 작업을 수행합니다. 데이터를 처리하고 연산을 수행하고 조회나 검색 등의 서비스를 제공하는 역할을 합니다.
- <span style="color:yellow">마스터는 워커를 조율하는 역할을 담당</span>하며 워커 장비에서 실행되는 서비스나 데이터에 대한 메타데이터를 관리하고, 워커 장비의 일부에 장애가 발생하더라도 서비스가 멈추지 않고 계속 실행될 수 있게 보장합니다.
- 마스터 장비는 장애 대비를 위해 일반적으로 2개 또는 3개로 구성되고, 워커장비는 훨씬 더 많은 개수로 구성할 수 있습니다.
- 클러스터에 워커를 추가하면 확장성을 높일 수 있고, 클러스터가 충분히 커지면 마스터도 추가합니다.
- 사용자나 다른 애플리케이션이 클러스터에 접근할 수 있게 하려면 게이트웨이 서버 또는 엣지 서버를 두고, 이 서버들은 오로지 설정 파일을 통해 클라이언트의 접근을 통제하는 작업만 수행하기도 합니다.

### 다음에선 컴포넌트를 간단하게 요약하고

### 컴포넌트 요약표
|프로젝트|설명|용도|의존 관계|
|--|--|--|--|
|주키퍼|분산 설정 서비스|분산 프로세스의 분산 잠금 사이의 메타데이터 공유|없음|
|HDFS|분산 파일 스토리지|<span style="color:yellow">불변 데이터를 위한 확장성 있는 스토리지|주키퍼</span>|
|얀|분산 리소스 스케줄링 및 실행 프레임워크|확장성 있는 분산 컴퓨팅 지원을 필요로 하는 프레임워크|주키퍼, HDFS|
|맵리듀스|범용 분산 연산 프레임워크| 배치, 분석 SQL, 스트리밍 작업| 자원 스케줄러(얀 또는 메소스(Mesos)) 및 데이터 소스(HDFS, 쿠두 등)|
|<span style="color:yellow">스파크</span>|범용 분산 연산 프레임워크|배치, 분석 SQL, 스트리밍 작업|자원 스케줄러(얀 또는 메소스)) 및 데이터 소스(HDFS, 쿠두 등)|
|하이브|SQL 기반 분석 쿼리 프레임워크|분석 SQL 작업|얀, 데이터 소스(HDFS, 쿠두 등)|
|임팔라|SQL 기반 대규모 병렬 처리 분석 엔진| 분석 및 대화형 SQL 작업|데이터 소스(HDFS, 쿠두, HBase)|
|HBase|위계 구조를 가진 키-값 데이터 분산/정렬 스토어| 정형 키(structured key)를 가진 로우(row) 기반 데이터에 대한 고속 랜덤 읽기/쓰기|HDFS, 주키퍼|
|쿠두|정형 데이터용 분산 스토어|랜덤 읽기/쓰기 및 분석 작업|없음|
|솔라|엔터프라이즈 검색 프레임워크|확장성 있는 도큐먼트 인덱싱 및 임의 필드 쿼리|HDFS, 주키퍼|
|카프카|분산 발행/구독 메시징 프레임워크|스트리밍 데이터의 확장성 있는 발행/구독|주키퍼|
|우지|워크플로우 스케줄러|데이터 프로세싱 파이프라인의 주기적 또는 실시간 실행|없음|


## 1. 핵심 컴포넌트 

## 1-1. HDFS(Hadoop Distributed File System)
- 확장성과 장애 허용성을 가진 분산 파일 시스템
순차 접근 방식을 통해 디스크에 불변 데이터를 저장하는 데 최적화되어 있다. 
- HDFS는 하둡 스택 내 다른 컴포넌트를 지원하는 핵심 기술이다.

## 1-2. 얀
- <span style="color:yellow">연산을 위한 중앙의 클러스터 매니저</span>
- 데이터를 확장성 있고 회복성 있는 방식으로 저장하는 것도 유용하지만 우리에게 가장 중요한 것은 데이터로부터 통찰을 얻어내는 것이다. 통찰을 얻으려면 데이터에 대한 연산이 필요하고, 하둡 파일시스템에 저장해야 할 정도로 큰 규모의 데이터에 대해서도 연산이 가능하도록 확장성이 필요하다. 그뿐 아니라 클러스터에 걸쳐 가용 자원의 사용 효율성을 높이고 데이터 접근 비용을 낮출 수 있도록 다양한 연산이 동시에 실행될 수 있어야 한다.
- 각 연산은 저마다 다른 크기의 데이터를 처리하며 서로 다른 크기의 연산력과 메모리를 필요로 한다.
- 한정된 자원에서 이런 요구사항을 충족시키려면 가용 연산 자원의 용량과 필요한 워크로드를 모두 알고 있는 중앙의 클러스터 매니저가 필요하다.
- 이런 목적으로 얀(YARN, Yet Another Resource Negotiator)이 설계되었다. <span style="color:yellow">얀은 각 워커 노드에 노드매니저(NodeManager) 데몬을 실행시키는데, 이 데몬이 마스터 프로세스인 리소스매니저(ResourceManager)에 여러 가지 정보를 보고한다.</span> 노드매니저는 가상 코어(vcore)의 단위로 얼마나 많은 연산 자원을 사용할 수 있는지와 해당 노드에 메모리가 얼마나 남아 있는지를 리소스매니저에게 알려준다. 

- 얀은 데이터를 직접 다루는 연산은 전혀 수행하지 않는다. 그래서 애플리케이션을 실행하는 프레임워크에 가깝다.

## 1-3. 아파치 주키퍼
- 컴퓨터 과학 분야에서 합의(consensus) 문제는 중요한 주제다.
- 애플리케이션이 여러 노드에 분산되는 경우 공유된 파라미터의 값에 대해 서로 다른 컴포넌트끼리 어떻게 동의할 수 있을까?

- 아파치 주키퍼는 하둡 에코시스템에 사용되는 <span style="color:yellow">회복성 있는 분산 설정 서비스</span>다. 주키퍼 안에서는 설정 데이터가 파일시스템과 지노드(znode)라는 노드 트리에 저장되고, 각 지노드는 데이터를 가지고 있으며 0개 이상의 자식 노드를 가질 수 있다. 클라이언트는 하나의 주키퍼 서버와 연결을 맺고 지노드를 생성/조회/수정/삭제할 수 있다.


## 2. 연산 프레임워크
## 2-1. 하둡 맵리듀스
- 하둡은 워래 맵리듀스(MapReduce)를 위해 만들어졌다. 맵리듀스는 구글 맵리듀스 논문(http://bit.ly/2QbTN6d)상의 설계를 자바로 구현한 애플리케이션이다. 처음에는 클러스터에서 실행되는 독립형(standalone)프레임워크였는데, 하둡 프로젝트가 점점 더 많은 애플리케이션과 다양한 용도에 맞게 진화함에 따라 얀에서 실행되는 애플리케이션으로 바뀌었다.
- 최근에는 아파치 스파크와 아파치 플링크(Flink)와 같은 새로운 엔진이 많이 사용되고 있지만, 아파치 하이브, 아파치 스쿱(Sqoop), 아파치 우지(Oozie), 아파치 피그(Pig)를 비롯한 많은 고수준 프레임워크들도 내부적으로는 맵리듀스 작업(job)으로 컴파일해서 연산을 실행하기 때문에 맵리듀스는 지금도 알아둘 필요가 있다.
- <span style="color:yellow">맵리듀스는 연산을 맵, 셔플, 리듀스 이렇게 3단계로 나눠서 처리한다. 맵 단계에서는 HDFS에서 읽은 데이터가 다수의 독립적인 맵 태스크(task)로 나뉘어서 병렬로 처리된다.</span>
- 맵 태스크는 이상적으로 데이터가 위치한 곳에서 실행되며, 일반적으로 HDFS 블록 하나당 하나의 맵 태스크가 배정되는 것이 이상적이다. 사용자는 소스 코드에 map() 함수를 정의하는데 이 함수는 파일에 있는 각 레코드를 키-값(key-value) 형태로 변환한 결과를 반환한다. 셔플 단계에서는 맵 단계의 결과물인 키-값이 맵리듀스에 의해 읽혀서 네트워크를 타고 리듀스 태스크의 입력값으로 전달된다. 사용자가 정의한 reduce() 함수는 하나의 키에 대한 여러 값을 집계 또는 결합(combine) 해서 입력 값보다 더 작은 개수의 결과값을 산출한다. 

![image](https://user-images.githubusercontent.com/51036842/188633883-35b9174a-a9e3-4432-af37-ec21f0810e9c.png)

- 위 그림에서 맵-셔플-리듀스 처리 과정의 예시를 볼 수 있다. mapper가 HDFS에 있는 파일을 레코드 단위로 읽고, 레코드는 ID 컬럼값을 키로 해서 셔플되고, 리듀서가 키 기준으로 레코드의 나머지 내용을 집계해서 결과를 다시 HDFS에 저장한다.

### 맵리듀스 단점
사용성
- 자바로 map() 함수와 reduce() 함수를 작성하고 컴파일하는 일은 버겁다.
- <span style="color:yellow">특출나게 효율적이지 않다.</span> 맵리듀스는 많은 양의 디스크 기반 I/O 를 수행하는데, 복잡한 처리 단계를 결합하거나 반복적인 연산을 할 때는 I/O 비용이 매우 많이 든다. 다중 단계 파이프라인은 HDFS I/O를 수반하는 개별 맵리듀스 작업을 조합해서 구성되는데, 처리 과정에서 찾아낼 수 있는 잠재적인 최적화가 전혀 고려되지 않는다.

- 하지만 서로 다른 장비에서 독립적으로 실행되는 다수의 태스크로 나눠 실행하는 맵, 그리고 맵의 결과를 셔플하고 그룹지어서 서로 다른 장비에서 집계하는 리듀스를 기반으로 데이터를 처리하는 맵리듀스의 개념적 토대는 SQL 기반 프레임워크를 비롯해 모든 분산 데이터 프로세싱 엔진의 근간을 이룬다. 아파치 스파크와 플링크, 임팔라는 세부적으로는 상당히 다르지만 맵리듀스 개념을 근간으로 만들어진 구현체라는 점은 모두 같다.

## 2-2 <span style="color:yellow">아파치 스파크</span>
- <span style="color:yellow">아파치 스파크는 효율성과 사용성에 중점을 둔 분산 연산 프레임워크로서, 배치 연산과 스트리밍 연산을 모두 지원한다.</span> 사용자가 데이터 조작 과정을 map()과 reduce() 함수를 이용해서 표현해야 하는 맵리듀스와 달리, <span style="color:yellow">스파크는 필터링, 조인, 그룹핑, 집계 같은 일반적인 연산을 특정 타입이나 스키마에 부합하는 데이터셋에 직접 적용할 수 있는 풍부한 API를 제공한다.</span> 이 API를 사용하면 복잡한 프로세싱 파이프라인도 쉽게 구성할 수 있다.

다음은 3개의 데이터셋을 스파크 API로 처리하는 예제이다.

![image](https://user-images.githubusercontent.com/51036842/188635604-fcece07f-ca8a-410c-ba5c-ae304ce6cfd9.png)

- 1개의 데이터 셋은 먼저 필터링(filter)을 거치고 나머지 2개의 데이터셋은 먼저 유니온(union)으로 합쳐진 후에 앞서 필터링을 거친 1개의 데이터셋과 조인(join)된다. 조인된 결과는 컬럼 기준으로 그룹지어져(groupBy) 집계된 후 저장된다. 데이터셋의 입출려은 HDFS나 쿠두(Kudu)를 사용해 배치 방식으로 수행될 수도 있고 카프카를 통해 스트림 방식으로 처리될 수도 있다.

- 스파크 데이터셋 연산의 주요 특징은 <span style="color:yellow">프로세싱 그래프가 표준 쿼리 옵티마이저를 거친 후에 실행된다</span>는 점이다. 스파크 표준 쿼리 옵티마이저는 관계형 데이터베이스 혹은 대규모 병렬 쿼리 엔진의 쿼리 옵티마이저와 상당히 비슷하다.

- <span style="color:yellow">옵티마이저는 파이프라인 실행의 효율성을 극대화하기 위해 프로세싱 그래프를 재배치(rearrange), 결합(combine), 가지치기(prune)</span> 할 수 있다. 이를 통해 <span style="color:yellow">데이터셋 연산의 효율을 큰 폭으로 높일 수 있으며</span> 맵리듀스의 문제점 중 하나였던 중간 과정의 과도한 I/O 발생도 피할 수 있다.

- <span style="color:yellow">스파크 설계의 주요 목표 중의 하나는 다수의 저사양 장비로 이루어지는 워커 노드에 있는 메모리를 최대한 활용하는 것이었다.</span> 메인 메모리에서 데이터를 읽고 쓰는 속도는 디스크에서 데이터를 읽고 쓰는 것과는 비교할 수 없을 정도로 빠른데, 이런 속도 차이를 통해 특정 유형의 작업 효율을 크게 높일 수 있다.
- 예를 들어 동일한 데이터셋에 대한 연산이 반복되는 분산 머신러닝 작업은 맵리듀스로 실행할 때보다 스파크로 실행할 때 훨씬 큰 성능 이득을 얻을 수 있다. 스파크는 데이터셋을 실행자(executor)의 메모리에 캐시하는데, 만약 데이터가 메모리에 다 저장될 수 없을 정도로 크다면 해당 파티션은 데이터를 디스크에 저장하거나 런타임에 재계산된다.

- <span style="color:yellow">스파크는 데이터셋에 대한 마이크로배치(microbatch)를 주기적으로 실행하는 방식으로 스트림 프로세싱(stream processing)을 구현</span>했다. 이 방식을 사용하면 배치 모드에서나 스트리밍 모드에서, 데이터에 적용되는 변환에 사용되는 코드를 거의 비슷하게 가져갈 수 있다.

- 또 스파크는 머신러닝에 사용할 수 있는 내장라이브러리와 API를 제공한다. 스파크 MLlib을 사용하면 데이터 준비, 클렌징(cleansing), 피처 추출(feature extraction), 알고리즘 실행 같은 머신러닝 모델 생성 과정을 분산 파이프라인을 통해 수행할 수 있다. 클러스터링, 분류, 회귀, 협업 필터링처럼 일반적으로 많이 사용되는 알고리즘에 대한 구현체가 스파크에 탑재되어 있다.
- 스파크는 데이터 프로세싱 분야에서는 이례적으로 강력한 프레임워크며 배치 프로세싱이나 머신러닝, 스트리밍 작업을 새로 만들 때는 사실상 표준으로 사용되고 있다. 하지만 스파크 외에도 배치나 스트림 프로세싱 용도로는 아파치 플링크(http://flink.apache.org), 대화형 SQL 용도로는 아파치 임팔라도 고려해볼 필요가 있다.

## 3. 분석용 SQL 엔진
- 맵리듀스와 스파크가 아주 유연하고 강력한 프레임워크이긴 하지만, 이를 사용하려면 자바나 스칼라, 파이썬 언어에 능숙해야 하고 터미널에서 코드를 배포하고 운영하는 데도 익숙해야 한다. 하지만 대부분의 기업에서는 여전히 SQL이 사실상 표준 분석 언어로 사용되는 것이 현실이고, 대부분의 분석 기법도 SQL을 기반을 한다.
- 그래서 하둡에 저장되는 <span style="color:yellow">정형 데이터를 SQL과 비슷한 인터페이스를 통해 사용할 수 있게 해주는 도구</span>를 만들었다.
- 이는 스토리지 엔진에 이미 존재하는 데이터를 쿼리하거나 대량 데이터를 저장하는 데 중점을 두며, 소규모의 트랜잭션 처리가 아니라 대규모의 분석에 특화된 엔진이 있다.

## 3-1. 아파치 하이브(Hive)
- 하이브는 HDFS에 저장된 정형 데이터를 SQL과 비슷한 문법을 가진 언어인 하이브QL로 쿼리할 수 있게 만든 최초의 기술이며 페이스북에서 개발되었다.
- 하이브QL을 사용하면 애플리케이션 코드를 컴파일하고 배포하지 않아도 데이터를 쿼리할 수 있다.
- 하이브에는 테이블 조인(join), 유니온(union), 서브쿼리(subquery), 뷰(view)처럼 SQL에서 공통적으로 사용하는 개념이 포함되어 있다.
- <span style="color:yellow">하이브는 사용자 쿼리를 파싱하고 최적화해서 하나 이상의 연쇄 배치 연산으로 컴파일하고, 이를 클러스터에서 실행한다.</span> 이런 연산은 일반적으로 맵 리듀스 작업(job)으로 실행되지만, 맵리듀스 대신에 아파치 테즈(Tez)나 스파크를 실행 엔진으로 사용할 수도 있다. 하이브는 2개의 주요 컴포넌트인 메타 데이터 서버와 쿼리 서버로 구성된다.

- 사용자는 하이브서버2(HiveServer2)라 불리는 쿼리 서버를 통해 SQL 쿼리를 실행할 수 있다. 사용자는 쿼리 서버와 세션을 먼저 맺고 하이브QL 문법에 맞는 쿼리를 쿼리 서버에 보낸다. <span style="color:yellow">하이브는 사용자가 보낸 쿼리를 파싱하고 최대한 최적화한 후 하나 이상의 배치 작업으로 컴파일한다</span>.
- 서브쿼리를 포함하고 있는 쿼리는 다중 단계(multistage)작업으로 컴파일되는데, <span style="color:yellow">각 단계에서 생성되는 중간 데이터는 HDFS 상의 임시 위치에 저장된다.</span> 하이브서버2는 다중 동시 접속을 지원하며 주키퍼의 공유 락(lock) 또는 독접 락(exclusive lock)을 통해 일관성을 보장한다. 
- <span style="color:yellow">쿼리 파서와 컴파일러는 쿼리 실행 계획을 만들기 위해 비용 기반 옵티마이저를 사용하며, 테이블 조인 시 최적의 전략을 선택하기 위해 메타스토어에 저장된 테이블 및 컬럼 통계 정보를 활용한다.</span> 하이브는 SerDes라는 직렬화/역직렬화 라이브러리가 내장되어 있어 다양한 포맷의 파일을 읽을 수 있으며, 확장을 통해 사용자 정의 파일 포맷도 읽을 수 있다.

![image](https://user-images.githubusercontent.com/51036842/188642978-30d49c93-d38a-4d55-8386-404e17ba17f1.png)

- 위 그림 1-8은 하이브 연산 방식을 개괄적으로 보여준다.
- 클라이언트는 사용자 세션 안에서 하이브서버2 인스턴스에게 쿼리를 보낸다. 하이브서버2는 하이브 메타스토어에서 쿼리에서 사용되는 데이터베이스와 테이블의 정보를 읽는다. 쿼리는 최적화되고 맵리듀스, 테즈, 스파크 등 여러 개의 작업으로 컴파일된다. 작업이 끝나면 하이브서버2를 통해 원격 클라이언트로 결과가 전송된다.

- 하이브는 LLAP를 통해 최근에 처리 속도가 많이 향상되었지만, 일반적으로 대화형 쿼리 엔진으로 분류되지 않고 많은 쿼리들은 결국에는 여러 맵리듀스 작업의 연쇄 실행을 통해 처리되며 완료되기까지 몇분~ 몇시간까지 소요된다. 그래서 <span style="color:yellow">하이브는 ETL(추출(Extract),변환(Transform),적재(Load)), 보고서 데이터 생성, 벌크 데이터 처리 등 오프라인 배치 작업에 더 적합하다.</span> 하이브 기반의 워크플로는 빅데이터 클러스터에서 높은 수준의 신뢰성을 확보하고 있으며 일반적으로 매우 견고하다. 스파크 SQL이 점점 더 많이 사용되고 있기는 하지만 하이브는 앞으로도 빅데이터 분야에서 필수적인 도구로 남을 것이다.

## 3-2 아파치 임팔라(Impla)
- 아파치 임팔라는 <span style="color:yellow">대규모 병렬 처리(Massive Parallel Processing) 엔진으로서, 하둡이나 클라우드 스토리지에 저장된 대용량 데이터셋에 대한 고속, 대화형 SQL 쿼리를 목적으로 설계</span>되었다.
- 그래서 테라바이트 단위의 데이터에 대해 <span style="color:yellow">다수의 동시 실행 애드혹(ad hoc) 쿼리, 보고서 스타일의 쿼리를 수초 내에 처리하는 것이 주요 설계 목표</span>다.
- 이를 통해 직접 SQL 쿼리를 작성하거나 비즈니스 인텔리전스 도구의 UI를 통해 쿼리를 실행하는 분석가(analyst)의 업무를 지원할 수 있다.

- 임팔라는 쿼리를 얀 리소스 매니저를 통해 실행되는 배치 작업으로 변환하는 하이브나 스파크 SQL 과는 다르다.
- 임팔라 데몬으로 불리는 워커 프로세스와 함께 C++로 구현된 독립적인 자체 서비스로 동작한다. 
- 하이브와는 달리, <span style="color:yellow">중앙 쿼리 서버가 없으며 각 임팔라 데몬이 사용자 쿼리를 받아서 쿼리를 실행하는 코디네이터 노드로 동작한다.</span>
- 사용자는 JDBC나 ODBC, 휴(Hue) 등의 UI나 터미널을 통해 쿼리를 실행하는 코디네이터 노드로 동작한다. 사용자는 JDBC나 ODBC, 휴(Hue) 등의 UI나 터미널을 통해 쿼리를 임팔라에 보낼 수 있다. 보내진 쿼리는 분산 쿼리 계획으로 컴파일된다. 분산 쿼리 계획은 여러 프래그먼트(fragment)로 나뉘는 연산자 트리(operator tree)로 구성된다.
- 각 프래그먼트는 트리 내에서 함께 실행될 수 있는 연산의 모음이다. 데몬은 다수의 프래그먼트 인스턴스를 클러스터 내에 있는 다른 데몬들에게 전송해서, 각 데몬이 로컬에 보유하고 있는 데이터에 대해 쿼리를 수행한다. 이 작업은 데몬 프로세스 안에서 하나 이상의 스레드에서 실행된다.

- 임팔라에서는 데이터가 분산 연산자 트리를 스트림으로 통과하면서 처리된다. 스캔노드에서 읽어진 레코드는 프래그먼트 인스턴스에 의해 처리되고, 교환 연산자를 통해 조인 그룹, 집계를 담당하는 다른 인스턴스로 스트리밍된다. 분산 프래그먼트 인스턴스로부터 나온 최종 결과는 코디네이터 데몬으로 다시 스트리밍되고, 코디네이터 데몬은 남아있는 최종 집계를 수행한 후에 사용자에게 최종 결과를 반환한다.

![image](https://user-images.githubusercontent.com/51036842/188646373-d4efde55-3b98-4cea-975a-06b9eeb4de09.png)

- 그림 1-9는 임팔라의 쿼리 프로세스이다. 클라이언트는 쿼리를 전송할 임팔라 데몬 서버를 선택한다. 이 코디네이터 노드는 쿼리를 원격 실행 프래그먼트로 컴파일하고 최적화해서 클러스터에 있는 다른 데몬에 전송하는데, 이를 쿼리 초기화(query initialization)라 한다. 전송받은 데몬은 프래그먼트에 들어있는 연산자에 따라 연산을 수행하고, 필요하다면 데몬끼리 레코드를 서로 교환하는데, 이를 분산 실행(distributed execution)이라 한다. 작업을 마치면 결과를 코디네이터로 스트리밍하고, 코디네이터는 최종 집계와 연산을 수행한 후에 클라이언트에게 결과를 스트리밍한다.

- 임팔라는 텍스트 파일, HBase 테이블, 아브로(Avro) 등 다양한 데이터 소스를 지원한다. 
- 선호하는 디스크 저장 포멧은 파케이(Parquet)이다.
- 파케이 파일은 데이터를 컬럼 단위로 저장하는데, 임팔라는 파케이 파일의 장점을 활용해서 쿼리에서 참조하는 컬럼의 데이터만 읽고, 참조되지 않는 불필요한 컬럼의 데이터는 읽지 않는다.
- 또한 임팔라는 사전 필터링(predicate pushdown)을 통해 데이터를 읽는 시점에서 필요한 레코드만 필터링해서 읽는다. 임팔라는 HDFS, 아파치 HBase, 아파치 쿠두, 아마존 S3, ADLS에 저장된 데이터를 읽을 수 있다.

### 추가 정보
- [클라우데라의 아파치 임팔라 가이드](http://bit.ly/2zijYyA)

## 4. 스토리지 엔진
- 하둡 에코시스템에서 사용되는 원조 스토리지 엔진은 HDFS다. HDFS는 순차 스캔으로 접근되며 삭제는 안 되고 추가만 가능한 대규모 데이터를 저장하는 데 아주 탁월하다. 
- 하지만 <span style="color:yellow"> HDFS는 임의의 레코드 조회나 수정, 문서 검색 등에는 좋지 않아서 이를 위한 프로젝트들이(Hbase, 쿠두, 솔라, 카프카) 개발</span>되었다.

## 4-1. 아파치 HBase
- 초기 인터넷 회사 중 일부는<span style="color:yellow"> 수백억에서 수조 개의 레코드를 저장하고 효율적으로 조회하고 수정하는 방법</span>을 필요로 했는데, 이런 수요가 아파치 HBase의 탄생으로 이어졌다.
- Hbase는 임의 접근(random-access) 가능한 키-값 구조의 반정형 데이터를 HDFS에 저장한다.
- HBase는 본질적으로 HDFS가 적합하지 않은 <span style="color:yellow">임의 접근 읽기/쓰기 작업을 HDFS가 적합한 순차 I/O 작업으로 변환하는 방법을 제공</span>한다.

- HBase는 관계형 데이터 스토어가 아니며, <span style="color:yellow">분산 테이블에 셀(Cell)이라 불리는 키-값 쌍의 반정형 데이터 형식으로 데이터를 저장</span>한다.
- HBase는 셀 키를 위계 구조를 가지도록 세분화해서, 관련된 셀은 함께 효율적으로 접근할 수 있다.
- 키의 첫 번째 부분은 로우 키(row key)라 하는데, 셀의 논리적인 그룹인 로우(row)를 정의하는 데 사용된다. 키의 나머지 부분은 컬럼 패밀리로 세분화되는 데, 컬럼 패밀리 역시 셀의 논리적 그룹을 나타낸다.
- 컬럼 패밀리는 메모리와 디스크에 분리되어 저장되고, 테이블 하나에 컬럼 패밀리는 일반적으로 몇 개 밖에 저장되지 않는다. 
- HBase는 타임스탬프 외에 키의 각 구성 요소와 값을 바이트 배열로 취급한다. 그래서 HBase는 셀의 어느 부분에 대해서도 타입 관련 정보를 모르며, 타입 관련 제약사항이 없으므로 결국 반정형 데이터 스토어라고 할 수 있다.

- HBase에서는 셀이 키 구성요소에 따라 정렬되어 저장된다. 먼저 로우 키 기준으로 정렬된 후 컬럼 패밀리, 컬럼 한정자, 마지막으로 타임스탬프 기준으로 정렬된다. HBase는 수평 파티셔닝을 사용한다. 그래서 테이블 안의 셀들은 파티션되어 클러스터에 분산 저장된다. 테이블의 로우 키를 위한 저장 공간도 리전이라는 파티션으로 나뉜다.
- 각 리전은 정렬된 로우 키를 중복 없이 나눠서 보유한다. 리전 사이의 경계는 리전 스플릿이라고 부른다. 예를 들어 로우가 임의의 알파벳 접두어를 가진다면 26개의 리전을 가진 테이블을 만들 수 있다. a로 시작하는 키를 가진 로우는 첫 번째 리전에 저장되고, c로 시작하는 로우 키를 가진 로우는 세 번째에 저장된다.
- 이는 수동으로 분산할 수 있고 테이블은 쉽게 분산될 수 있으며 높은 확장성을 가지게 된다.

- 테이블과 셀 키의 설계는 성능에 절대적인 영향을 미치므로 매우 중요하다. 
- 테이블 레이아웃을 바르게 설계하려면 HBase의 작동 방식을 실무적으로 잘 이해해야 한다. 그렇지 않으면 결국 테이블 풀 스캔, 리전 핫스팟, 잦은 압축같은 좋지 않은 결과로 이어진다.
- <span style="color:yellow">HBase는 상대적으로 작은 범위 스캔을 거친 작은 그룹의 셀에 대해 적절하게 분산된 읽기/쓰기 요청을 처리하는 임의 접근 I/O 처리에 뛰어나다.</span> 그래서 분석 작업에서 처리하는 수준의 대규모의 데이터를 HBase로 스캔하면 실행 시간이 오래 걸릴 수 있으며, 이런 작업은 직접 HDFS 파일을 대상으로 수행하는 것이 낫다.

## 4-2 아파치 쿠두
- 전통적인 하둡 기반 아키텍처에서 가장 힘든 부분 중 하나는, 분석 작업에서의 높은 처리량과 동일한 데이터를 읽을 때 빠른 임의 접근 읽기를 모두 지원하려면 여러가지 스토리지 엔진을 사용해야 한다는 점이다. 임의 접근 쿼리를 위해서는 HBase나 어큐뮬로 같은 스토리지 엔진이 필요하고, 분석 작업에는 HDFS, 파케이, 임팔라, 스파크 SQL, 하이브가 필요하다. 결국 데이터 입수 과정과 오케스트레이션 파이프라인을 복잡하게 만든다.
- 그래서 <span style="color:yellow">임의 접근과 순차 스캔을 모두 지원하고 기존 데이터 수정까지 허용하는 스토리지 엔진과 쿼리 엔진</span>을 만들었고 이것이 <span style="color:yellow">쿠두</span>이다.
- 쿠두는 <span style="color:yellow">미리 정의된 스키마를 따르는 테이블에 타입을 가진 컬럼을 구성된 레코드를 저장하는 정형 데이터 스토어</span>이다.
- 컬럼의 일부는 테이블의 기본키(PK)로 사용되며, 레코드를 빨리 탐색할 수 있도록 인덱스가 생성된다. 쿠두는 생성, 수정, 업서트(upsert), 삭제를 지원한다. 읽기 작업에 컬럼 추출(column projection)을 이용할 수 있고 컬럼 값을 기준으로 조건을 두어 필터링도 가능하다.
- 쿠두는 수평 파티셔닝을 통해 테이블을 클러스터에 분산 저장한다. 테이블은 두 가지 파티셔닝 메커니즘 중의 하나 또는 둘 모두를 조합하는 방법을 통해 태블릿으로 나뉜다. 레코드는 하나의 테블릿 안에만 존재할 수 있으며 기본 키 컬럼에 대해 정렬된 인덱스를 관리한다. 
- 첫번째 파티셔닝 메커니즘은 HBase나 어큐뮬로 사용자에게 익숙한 범위 파티셔닝이다. 각 태블릿은 상한과 하한으로 이뤄진 범위를 갖고 있으며, 범위 내에 들어가는 파티션 키를 가진 모든 레코드는 해당 태블릿에 저장된다.

- 두번째 파티셔닝 매커니즘은 해시 파티셔닝이다. 사용자는 테이블 파티셔닝의 기준이 되는 고정된 수의 해시 버킷을 지정할 수 있고, 각 행의 해시 값 계산에 사용되는 하나 이상의 컬럼을 선택할 수 있다. 쿠두는 각 행에서 선택된 컬럼의 해시 값을 해시 버킷의 수로 나눈 나머지를 기준으로 파티셔닝해서 해당 행을 태블릿에 저장한다.

- 두 가지 파티셔닝 메커니즘을 조합한 멀티레벨 파티셔닝도 가능하다.

- 쿠두의 일반적인 용도는 다음과 같다.
    - IoT 데이터셋 같은 <span style="color:yellow">대규모 시계열 메트릭</span>
    - 스타 스키마 테이블에 대한 OLAP(OnLine Analytical Processing) 분석 같은 <span style="color:yellow">대규모 가변 데이터셋에 대한 리포팅</span>

- 어떤 스토리지나 쿼리 엔진을 사용하든, 적절한 스키마, 테이블 레이아웃을 선택하는 것이 효율적인 연산에 매우 중요하다.

### 참고 자료
- [공식 문서](https://bit.ly/2Tv16os)

## 4-3 아파치 솔라
- 비정형 또는 반정형 데이터에 대해 더 유연한 검색이 필요할 때는 SQL만으로 충분하지 않을 수 있다. <span style="color:yellow">로그 검색, 문서 보관소, 사이버 보안 분석 등의 작업에서는 텍스트 검색, 퍼지 검색, 패싯 검색, 음소 검색, 동의어 매칭, 공간 검색 등을 활용해서 데이터를 조회</span>하기도 한다. 이런 엔터프라이즈 검색을 지원하려면 수십억 개의 문서와 수백 테라바이트의 데이터를 <span style="color:yellow">자동으로 처리하고 분석하고 인덱스를 만들고 쿼리</span>할 수 있어야 한다. 
- 현재 하둡 에코 시스템에는 이를 지원하는 두 가지 주요 기술이 있는데, <span style="color:yellow">바로 아파치 솔라와 엘라스틱서치</span>다.
- 엔터프라이즈 검색 용도로는 두 가지 모두 심도 있게 알아볼 가치가 있다.

- 다양한 검색을 지원하기 위해 솔라는 아파치 루씬에서 나온 개념인 역방향 인덱스를 사용한다. 역방향 인덱스는 매칭되는 도큐먼트(document) 목록에 용어(term)를 매핑한다.
- 단어(word), 스템(stem), 범위(range), 숫자(number), 좌표(coordinate) 등이 이 용어에 해당한다.
- 도큐먼트는 필드를 포함한다. 필드는 도큐먼트에서 사용되는 용어의 타입을 정의한다. 
- 필드는 개별 토큰으로 분리될 수 있고 각 필드별로 인덱스를 따로 정의할 수 있다.
- 도큐먼트에 포함되는 필드는 스키마로 정의된다.

- <span style="color:yellow">솔라의 인덱싱 처리와 스토리지 구조는 도큐먼트 조회 순위를 구하는 데 적합하다.</span> 많은 고급 쿼리 파서가 일치 매칭, 퍼지 매칭, 정규 표현식 매칭 등을 수행할 수 있고, 어떤 주어진 쿼리에 대해서 인덱스 탐색기는 쿼리의 조건식을 만족하는 도큐먼트를 조회한다.
- 도큐먼트는 점수가 매겨지고 정해진 기준에 따라 정렬될 수도 있다.
- 기본적으로 가장 높은 점수가 매겨진 도큐먼트가 가장 먼저 반환된다.
- <span style="color:yellow">솔라는 인덱스 관리 기능과 유연하고 조합 가능한 쿼리 기능, 인덱싱 파이프라인 등을 제공하는 Restful 서비스</span>로 루씬 라이브러리를 내부적으로 감싸고 있다.
- SolrCloud 기능을 사용하면 <span style="color:yellow">논리 인덱스(logical index)를 여러 장비에 걸쳐 분산할 수 있고, 이를 통해 확장성 높은 쿼리 프로세싱과 인덱싱이 가능하다.</span> 솔라는 인덱스 파일을 회복성 있고 확장성 있는 스토리지에 저장하기 위해 HDFS에 저장할 수도 있다.
- 솔라는 도큐먼트를 Collection에 저장한다. 컬렉션은 사용자가 정의한 필드와 타입을 담고 있는 미리 정의된 스키마를 통해 생성된다.
- 임의의 이름을 포함하는 도큐먼트를 처리하는 데 동적 필드를 사용할 수 있고, 동적 필드를 사용하면 어떤 네이밍 패턴에 매칭되는 도큐먼트 필드에 어떤 타입을 사용할지 정의할 수 있다.
- 추가로 솔라 컬렉션은 스키마 없이도 제공된 필드의 타입을 추측해서 도큐먼트에 추가할 수 있는 스키마리스 모드로 동작할 수도 있다.

- <span style="color:yellow">솔라클라우드는 컬렉션을 파티셔닝하고 여러 대의 솔라 서버에 분산하여 수십억 개의 도큐먼트를 저장하고 쿼리 동시 실행으로 쿼리 효율을 높일 수 있다.</span>
- 다른 스토리지나 쿼리 엔진과 마찬가지로, 솔라도 장점과 단점이 모두 있다. 일반적으로 솔라클라우드를 바르게 설정해서 배포하면 수십억 개의 도큐먼트를 가진 분산 컬렉션을 사용할 수 있지만, 쿼리와 인덱싱 부하를 적절히 분산하는 데 매우 세심한 주의를 기울여야 한다.
- <span style="color:yellow">솔라의 장점은 유연한 쿼리 문법과 수백만개의 도큐먼트에 대한 복잡한 쿼리에 대해 수십 개에서 수백 개의 결과 도큐먼트를 1초 이내에 반환할 수 있다</span>는 점이다.
- 다만 솔라는 일반적으로 수백만 개의 도큐먼트를 한 번에 반환하는 대규모 분석 용도로 적합하지 않다.
- 솔라도 컬렉션 쿼리에 SQL 문법을 지원한다.
### 참고자료
- 공식 문서 읽어보기를 추천 [공식 문서](https://bit.ly/2zj4jz8) 
- http://yonik.com : 요닉 실리의 블로그로 솔라의 다양한 기능에 대한 풍부한 배경 지식과 비공식 솔라 가이드를 얻을 수 있다.

## 4-4 아파치 카프카
- 클러스트럴 사용하는 주된 이유 중의 하나는 많은 데이터 소스에서 오는 데이터를 저장하고 처리할 하나의 플랫폼을 구축하기 위해서다. 엔터프라이즈 환경에는 웹 로그, 장비 로그, 비즈니스 이벤트, 트랜잭션 데이터, 텍스트 문서, 이미지 등 다양한 데이터 소스가 있다. 푸쉬 기반, 풀 기반, 배치, 스트림 등 데이터가 수집되는 방법도 다양하며, HTTP, SCP/SFTP, JDBC, AMQP, JMS 등 프로토콜도 여러 가지다.
- 플랫폼 에코시스템에서는 HDFS, HBase, 엘라스틱서치, 쿠두 등 데이터가 출력되는 곳도 다양하다. 이 모든 환경을 지원하면서 플랫폼으로의 데이터 입수를 관리하고 오케스트레이션하다 보면 설계의 운영상의 악몽이 되기 쉽다.
- 특히 <span style="color:yellow">스트리밍 데이터에 대해서는, 현재 사용되는 메시지 브로커 기술로는 빅데이터 처리 수요를 감당할 수 있는 확장성을 확보하는 데 어려움이 있다.</span> 그 중에서도, <span style="color:yellow">넓은 대역폭을 통해 데이터를 읽고 쓰기를 원하고, 스트림 내에서 자신들의 위치를 유지하려고 하는 수백 개의 클라이언트를 모두 지원해야 한다는 점이 가장 어려운 점으로 손꼽힌다.
- 메시지 브로커 기술을 통해 확장성 있는 방식으로 데이터 전달을 보장하기란 쉽지 않으며, 폭발적으로 많이 입수되는 대용량 스트림이나 장애 발생 프로세스로부터 나오는 데이터 백로그를 처리하는 것도 어려운 일이다. <span style="color:yellow">이러한 상황을 극복하기 위해 개발된 것이 아파치 카프카</span>이다.

- <span style="color:yellow">아파치 카프카는 용량과 읽기/쓰기 대역폭을 수평적으로 확장할 수 있도록 설계된 발행/구독 시스템이다. 카프카는 입수되는 메시지를 분산 저장되는 순차적인 로그로 저장하고, 클라이언트나 클라이언트의 그룹은 간단한 숫자 형태의 오프셋으로 특정 지점으로부터의 데이터를 가져올 수 있다. </span>
- 카프카는 다양한 업스트림 소스와 다운스트림 싱크를 통합하고, 회복성 있는 고가용성 입수 버퍼를 제공하면서 핵심적인 통합 기술로 자리매김했다. 카프카 에코시스템 자체적으로 스트림 프로세싱이나 스트림의 상태 있는 쿼리(statful query of stream)를 점점 더 폭넓게 지원하면서 카프카를 기록 시스템으로 활용하는 사례도 늘고 있다.

- <span style="color:yellow">카프카의 기초가 되는 데이터 구조는 토픽(topic)이다. 토픽은 여러 대의 서버(카프카에서는 broker라고 한다)에 분산되는 일련의 메시지(또는 레코드)다. 각 토픽은 여러 개의 파티션으로 나뉠 수 있으며 각 파티션에 저장되는 데이터는 디스크에 로그로 저장된다.</span>
- 회복성을 확보하기 위해 파티션은 다수의 다른 브로커에 복제된다.
- 카프카에 저장되는 메시지는 바이트 배열로 된 키/값 쌍을 구성된다. 클라이언트는 프로듀서(producer)를 통해 카프카 토픽의 파티션으로 메시지를 발행한다. 토픽의 각 파티션은 정렬된 불변 로그다. 새 메시지는 로그의 마짐가에 순차적으로 추가되므로 I/O 효율이 높다. 파티션 안에 저장되는 각 메시지는 순차적으로 증가하는 오프셋을 가진다. 클라이언트는 컨슈머를 통해 토픽에 있는 데이터를 읽을 수 있다.
- 확장성을 위해 여러 컨슈머를 하나의 컨슈머 그룹으로 묶을 수도 있고 컨슈머는 마지막으로 데이터 읽기에 성공했을 때의 오프셋을 알고 있으므로 그다음 오프셋에 있는 데이터부터 누락 없이 다시 읽을 수 있다.

- 카프카는 매우 다양한 용도로 사용될 수 있다. 그중에서도 하둡 스토리지 엔진에서 데이터를 스트리밍하는 확장성 있는 버퍼 용도로 가장 많이 사용된다. 또한 카프카 커넥트, 아파치 플룸이나 스파크 스트리밍 같은 시스템은 데이터를 읽어서 처리하고 결과를 새로운 토픽에 저장하는 유연한 스트림 프로세싱 과정에서 데이터 교환 버스로도 자주 사용된다.

### 더 알아보기
- [아파치 공식 문서](https://kafka.apache.org/documentation/)
- 카프카 핵심 가이드
- I Heart Logs

## 5. 데이터 입수(데이터 가져오기)
- 데이터 입수(ingestion) 분야에도 많은 기술이 사용된다. 
- 전통적으로 사용되는 2가지 주요 데이터 입수 기술은 스트림 데이터를 확장성 있는 방법으로 입수하는 데 중점을 둔 아파치 플룸과 관계형 데이터베이스에 있는 데이터를 가져오거나 내보내기하는 데 중점을 둔 아파치 스쿱(Sqoop)이다. 그 외에도 데이터 입수 파이프라인 처리를 단순화하는 데 사용되는 기술이나 직접적인 코딩을 줄여주는 기술 등 다양한 도구가 만들어 졌는데 그중 관심가져 볼만한 것은 2가지이다.
- [아파치 나이파이](https://nifi.apache.org/)
- [스트림셋 데이터 컬렉터](https://github.com/streamsets/datacollector-docker)

## 6. 오케스트레이션
- 데이터 입수 배치 작업과 데이터 분석 파이프라인은 독립적인 여러 단계로 구성되며, 각 단계마다 각기 다른 기술이 사용될 수 있다. 이런 파이프라인 작업에 대한 오케스트레이션 및 스케줄링과 복잡한 상호 의존 관계를 나타낼 방법이 필요하다.
## 6-1. 아파치 우지
- 아파치 [우지](https://oozie.apache.org)는 하둡에서 사용되는 작업 스케줄링 및 실행 프레임워크다.
- 우지 작업의 기본 실행 단위는 액션(action)이며, 하이브 쿼리나 맵리듀스 작업처럼 하둡 에코시스템에서 실행되는 태스크를 나타낸다. 연쇄적으로 함께 실행돼야 하는 액션을 묶어서 워크플로우를 구성할 수도 있다.
- 워크플로우는 코디네이터를 통해 스케줄링되고, 코디네이터는 애플리케이션을 논리적으로 묶기 위해 그룹지어져서 번들을 구성한다.
- 우지 작업은 워크플로우나 코디네이터 또는 번들을 의미한다.

- 우지 작업은 XML 파일로 정의된다. 워크플로우는 액션으로 구성되는 유향 비순환 그래프를 포함하므로 처리 과정이 기본적으로 플로우차트와 비슷하다고 볼 수 있다.
- 코디네이터는 시간 간격이나 입력 데이터셋의 가용성을 기준으로 워크플로우의 실행 시점을 스케줄링한다.
- 번들은 관련 있는 여러 코디네이터를 그룹지어서 정해진 시간에 실행할 수 있게 해준다.

- 우지 작업이 우지 서버에 제출되면 우지 서버는 우지 작업을 정의하는 XML 파일을 검증하고 작업 수명주기를 관리한다. 작업의 종류에 따라 수행되는 동작도 다르다.
- 우지 서버는 워크플로우 작업을 처리할 때는 하둡 클러스터에서 실행되는 개별 액션을 추적하고, 워크플로우가 정상 종료되거나 에러가 발생할 때까지 그래프를 구성하고 있는 액션을 실행한다. 코디네이터를 처리할 때는 정해진 스케줄에 따라 실행될 수 있도로 워크플로우를 준비하고, 워크플로우가 실행되는 인스턴스에 필요한 데이터가 준비되어 있는지 검사하고, 데이터가 준비돼 있지 않으면 준비될 때까지 워크플로우의 실행을 미뤄둘 수 있다. 번들을 처리할 때는 번들에 정의된 코디네이터를 각각 실행한다.

- 워크플로우 액션은 비동기 액션과 동기 액션으로 나눌 수 있다. 대부분의 액션은 런처를 통해 얀 위에서 비동기 방식으로 실행된다. 런처는 리듀스 없이 맵 태스크만 포함하는 맵리듀스 작업으로서 다른 하둡 작업(스파크나 맵리듀스 또는 하이브)을 하둡 클러스터에 전송할 수 있다.
- 우지 서버가 직접 하둡 작업을 실행하지 않고 하둡 클러스터에 위임하는 아키텍처 덕분에 우지 서버는 가벼울 수 있으며, 결론적으로 수백 개의 액션을 동시에 쉽게 실행할 수 있다. 
- 또한 작업 상태가 데이터베이스에서 관리되므로 우지 서버는 실행 중인 액션에 영향을 미치지 않으면서 재시작 후에도 어디부터 작업을 다시 시작해야 할지 알 수 있다. 이를 통해 우지 서버에 장애가 발생하더라도 오랫동안 실행되고 있는 애플리케이션은 영향을 받지 않는다. 이메일 전송이나 몇 가지 HDFS 명령어 등 충분히 가벼워서 얀을 통해 실행되지 않아도 된다고 판단되는 액션은 우지 서버에서 동기 방식으로 실행되기도 한다. 우지 작업 정의 및 관련된 모든 파일과 라이브러리는 반드시 HDFS에 저장돼야 한다. 또 일반적으로 애플리케이션마다 별도의 디렉토리를 두고 그 안에 저장한다. 
- 우지 서버는 멀티스레드 웹 서버를 통해 사용자가 작업을 전송하고 모니터링하고 제어할 수 있는 HTTP API를 제공한다.
### 함께 보기
- 모든 이가 우지를 좋아하는 편이 아니며, 우지보다 더 유연하고 사용성이 좋은 경쟁주자들이 부상하고 있다. 아직 오케스트레이션 도구를 사용하지 않았다면 다음 도구도 고려해보면 좋다.
- [아파치 에어플로우](https://airflow.apache.org/)
- 스포티파이에서 만든 [루이지(Luigi)](https://github.com/spotify/luigi)

